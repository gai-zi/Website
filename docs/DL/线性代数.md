---
sidebar_position: 5

---

# 线性代数

:::tip

基础内容请见[Linear Algebra](https://website-gai-zi.vercel.app/docs/Computer-Graphics/Linear Algebra)

:::

> API [linear-algebra - Jupyter Notebook](http://localhost:8888/notebooks/Downloads/d2l-zh/pytorch/chapter_preliminaries/linear-algebra.ipynb)

## 张量算法

**张量算法的基本性质**

标量、向量、矩阵和任意数量轴的张量（本小节中的“张量”指代数对象）有一些实用的属性。
例如，从按元素操作的定义中可以注意到，任何按元素的一元运算都不会改变其操作数的形状。
同样，**给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量**。
例如，将两个相同形状的矩阵相加，会在这两个矩阵上执行元素加法。

```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = A.clone()  # 通过分配新内存，将A的一个副本分配给B
A + B
A * B
```

## 矩阵算法

```python
import torch

A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
print(A)

#降维
#sum第一维度
A_sum_axis0 = A.sum(axis=0)
print(A_sum_axis0, "\n",  A_sum_axis0.shape)
#sum第二维度，汇总所有列的元素降维
A_sum_axis1 = A.sum(axis=1)
print(A_sum_axis1, "\n",  A_sum_axis1.shape)

#求均值
A.mean()
A.sum() / A.numel()
#计算平均值的函数也可以沿指定轴降低张量的维度
A.mean(axis=0)
A.sum(axis=0) / A.shape[0]

#非降维求和
C = torch.arange(20, dtype=torch.float32).reshape(5, 4)
sum_C = C.sum(axis=1, keepdims=True)
print(C)
print(sum_C)
print(A/sum_C)

#累加求和
A.cumsum(axis=0)

#点积
x = torch.arange(4, dtype = torch.float32)
y = torch.ones(4, dtype = torch.float32)
print(x, y, torch.dot(x, y))

#矩阵-向量积
#(5*4)*(4*1)=(5*1)
torch.mv(A, x)

#矩阵-矩阵乘法
#(5*4)(4*3)=(5*3)
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = torch.ones(4, 3)
D = torch.mm(A, B)
print(A)
print(B)
print(D)
```

### 范数

线性代数中最有用的一些运算符是*范数*（norm）。
非正式地说，向量的*范数*是表示一个向量有多**大**。
这里考虑的*大小*（size）概念不涉及维度，而是分量的大小。

在线性代数中，向量范数是将向量映射到标量的函数$f$。
给定任意向量$\mathbf{x}$，向量范数要满足一些属性。

1. 如果我们按常数因子$\alpha$缩放向量的所有元素，
   其范数也会按相同常数因子的*绝对值*缩放：$$f(\alpha \mathbf{x}) = |\alpha| f(\mathbf{x}).$$

2. 熟悉的三角不等式: $$f(\mathbf{x} + \mathbf{y}) \leq f(\mathbf{x}) + f(\mathbf{y}).$$

3. 简单地说范数必须是非负的: $$f(\mathbf{x}) \geq 0.$$

任何东西的最小的*大小*是0。

个性质要求范数最小为0，当且仅当向量全由0组成。

$$\forall i, [\mathbf{x}]_i = 0 \Leftrightarrow f(\mathbf{x})=0.$$

**范数听起来很像距离的度量。**
欧几里得距离是一个$L_2$范数：
假设$n$维向量$\mathbf{x}$中的元素是$x_1,\ldots,x_n$，其[**$L_2$*范数*是向量元素平方和的平方根：**]

(**$$\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2},$$**)

---

其中，在$L_2$范数中常常省略下标$2$，也就是说$\|\mathbf{x}\|$等同于$\|\mathbf{x}\|_2$。
在代码中，我们可以按如下方式计算向量的**$L_2$范数**。

```python
u = torch.tensor([3.0, -4.0])
torch.norm(u)
```

---

深度学习中更经常地使用$L_2$范数的平方，也会经常遇到[**$L_1$范数，它表示为向量元素的绝对值之和：**]

(**$$\|\mathbf{x}\|_1 = \sum_{i=1}^n \left|x_i \right|.$$**)

与$L_2$范数相比，$L_1$范数受异常值的影响较小。
为了计算$L_1$范数，我们将绝对值函数和按元素求和组合起来。

```python
torch.abs(u).sum()
```

$L_2$范数和$L_1$范数都是更一般的$L_p$范数的特例：

$$\|\mathbf{x}\|_p = \left(\sum_{i=1}^n \left|x_i \right|^p \right)^{1/p}.$$

类似于向量的$L_2$范数，[**矩阵**]$\mathbf{X} \in \mathbb{R}^{m \times n}$(**的*Frobenius范数*（Frobenius norm）是矩阵元素平方和的平方根：**)

(**$$\|\mathbf{X}\|_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n x_{ij}^2}.$$**)

Frobenius范数满足向量范数的所有性质，它就像是矩阵形向量的$L_2$范数。
调用以下函数将计算矩阵的Frobenius范数。

```python
torch.norm(torch.ones((4, 9)))
```

